{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "简答题：\n",
    "1. 如果你的训练集具有数百万个特征，那么可以使用哪种线性回归训练算法？\n",
    "\n",
    "2. 如果你的训练集里特征的数值大小迥异，那么哪些算法可能会受到影响？受影响程度如何？你应该怎么做？\n",
    "\n",
    "3. 训练逻辑回归模型时，梯度下降可能会卡在局部最小值中吗？\n",
    "\n",
    "4. 如果你让它们运行足够长的时间，是否所有的梯度下降算法都能得出相同的模型？\n",
    "\n",
    "5. 假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，那么可能是什么情况？你该如何解决？\n",
    "\n",
    "6. 当验证误差上升时立即停止小批量梯度下降是个好主意吗？\n",
    "\n",
    "7. 哪种梯度下降算法（在我们讨论过的算法中）将最快地到达最佳解附近？哪个实际上会收敛？如何使其他的也收敛\n",
    "\n",
    "8. 假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？\n",
    "\n",
    "9. 假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？\n",
    "\n",
    "10. 为什么要使用：a.岭回归而不是简单的线性回归（即没有任何正则化）？b.Lasso而不是岭回归？c.弹性网络而不是Lasso回归？\n",
    "\n",
    "11. 假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑回归分类器还是一个softmax回归分类器？"
   ],
   "id": "6af774aa03256f6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "编程题：",
   "id": "10695eaf8992f30e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-04T14:26:42.529550Z",
     "start_time": "2025-08-04T14:26:42.513938Z"
    }
   },
   "source": [
    "# todo 编程题: 在不使用sklearn的情况下，仅使用Numpy，为softmax回归实现带早停的批量梯度下降，将它用于分类任务，\n",
    "#  例如鸢尾花数据集  load_iris, 只用两个特征就可以：\"petal width (cm)\", \"petal length (cm)\"\n",
    "#  强调：除了读数据，其他全用numpy （包括分离测试+验证），不用sklearn\n",
    "\n",
    "#  注意：\n",
    "#  1. 要实现l2正则化\n",
    "#  2. 除了数据读取，其他仅使用numpy，包括训练集+验证集分离，以及softmax预测 和 损失计算"
   ],
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:24:30.162418Z",
     "start_time": "2025-08-10T06:24:30.136454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 1. 加载数据\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal width (cm)\", \"petal length (cm)\"]].values# 只使用 petal length 和 petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# 2. One-Hot 编码\n",
    "num_classes = 3\n",
    "y_one_hot = np.eye(num_classes)[y]\n",
    "\n",
    "# 3. 特征归一化\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_normalized = (X - X_mean) / X_std\n",
    "\n",
    "# 4. 数据集划分\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data[train_indices], data[test_indices]\n",
    "X_train, X_val = shuffle_and_split_data(X_normalized, 0.2)\n",
    "y_train, y_val = shuffle_and_split_data(y_one_hot, 0.2)"
   ],
   "id": "e570eec2e1cc8291",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T07:10:46.084169Z",
     "start_time": "2025-08-10T07:10:45.639450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Softmax 函数\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "# 6. 损失函数（带 L2 正则化）\n",
    "def compute_loss(X, y_true, W, b, reg_lambda):\n",
    "    m = X.shape[0]\n",
    "    Z = X @ W + b\n",
    "    y_pred = softmax(Z)\n",
    "    cross_entropy = -np.sum(y_true * np.log(y_pred)) / m\n",
    "    l2_penalty = reg_lambda * np.sum(W ** 2)\n",
    "    return cross_entropy + l2_penalty\n",
    "\n",
    "# 7. 梯度计算\n",
    "def compute_gradients(X, y_true, y_pred, W, reg_lambda):\n",
    "    m = X.shape[0]\n",
    "    dW = (X.T @ (y_pred - y_true)) / m + 2 * reg_lambda * W\n",
    "    db = np.sum(y_pred - y_true, axis=0, keepdims=True) / m\n",
    "    return dW, db\n",
    "\n",
    "# 8. 模型初始化\n",
    "input_dim = X_train.shape[1]\n",
    "W = np.random.randn(input_dim, 3) * 0.01\n",
    "b = np.zeros((1, 3))\n",
    "\n",
    "# 9. 训练参数\n",
    "learning_rate = 0.1\n",
    "reg_lambda = 0.01\n",
    "n_epochs = 10000\n",
    "patience = 100\n",
    "best_loss = float('inf')\n",
    "no_improvement_count = 0\n",
    "min_delta = 1e-6\n",
    "# 10. 训练循环（带早停）\n",
    "for epoch in range(n_epochs):\n",
    "    Z_train = X_train @ W + b\n",
    "    y_train_pred = softmax(Z_train)\n",
    "    dW, db = compute_gradients(X_train, y_train, y_train_pred, W, reg_lambda)\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    train_loss = compute_loss(X_train, y_train, W, b, reg_lambda)\n",
    "    val_loss = compute_loss(X_val, y_val, W, b, reg_lambda)\n",
    "\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        best_W = W.copy()\n",
    "        best_b = b.copy()\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}, best validation loss: {best_loss}\")\n",
    "        break\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# 11. 准确率评估\n",
    "def accuracy(X, y, W, b):\n",
    "    Z = X @ W + b\n",
    "    y_pred = np.argmax(softmax(Z), axis=1)\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "train_acc = accuracy(X_train, y_train, best_W, best_b)\n",
    "val_acc = accuracy(X_val, y_val, best_W, best_b)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")"
   ],
   "id": "964afda7527ba644",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.0455, Val Loss: 1.0413\n",
      "Epoch 100, Train Loss: 0.4423, Val Loss: 0.3989\n",
      "Epoch 200, Train Loss: 0.3923, Val Loss: 0.3477\n",
      "Epoch 300, Train Loss: 0.3760, Val Loss: 0.3301\n",
      "Epoch 400, Train Loss: 0.3697, Val Loss: 0.3229\n",
      "Epoch 500, Train Loss: 0.3670, Val Loss: 0.3196\n",
      "Epoch 600, Train Loss: 0.3658, Val Loss: 0.3180\n",
      "Epoch 700, Train Loss: 0.3653, Val Loss: 0.3172\n",
      "Epoch 800, Train Loss: 0.3650, Val Loss: 0.3167\n",
      "Epoch 900, Train Loss: 0.3649, Val Loss: 0.3165\n",
      "Epoch 1000, Train Loss: 0.3648, Val Loss: 0.3163\n",
      "Epoch 1100, Train Loss: 0.3648, Val Loss: 0.3162\n",
      "Epoch 1200, Train Loss: 0.3648, Val Loss: 0.3161\n",
      "Epoch 1300, Train Loss: 0.3648, Val Loss: 0.3161\n",
      "Epoch 1400, Train Loss: 0.3648, Val Loss: 0.3161\n",
      "Epoch 1500, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 1600, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 1700, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 1800, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 1900, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 2000, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 2100, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 2200, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 2300, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 2400, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Epoch 2500, Train Loss: 0.3648, Val Loss: 0.3160\n",
      "Early stopping at epoch 2559, best validation loss: 0.3159883162775074\n",
      "Train Accuracy: 0.9417\n",
      "Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
