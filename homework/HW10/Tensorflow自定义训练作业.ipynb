{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 简答题",
   "id": "900a6e19bb0fa847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. TensorFlow 是否可以简单替代 NumPy？两者之间的主要区别是什么？\n",
    "\n",
    "2. 使用 `tf.range(10)` 和 `tf.constant(np.arange(10))` 是否会得到相同的结果？\n",
    "\n",
    "3. 可以通过编写函数或继承 `tf.keras.losses.Loss` 来定义自定义损失函数。两种方法分别应该在什么时候使用？\n",
    "\n",
    "4. 可以直接在函数中定义自定义指标或采用 `tf.keras.metrics.Metric` 子类。两种方法分别应该在什么时候使用？\n",
    "\n",
    "5. 什么时候应该自定义层而不是自定义模型？\n",
    "\n",
    "6. 有哪些示例需要编写自定义训练循环？\n",
    "\n",
    "7. 自定义 Keras 组件中可以包含任意 Python 代码，还是必须转换为 TF 函数？\n",
    "\n",
    "8. 如果要将函数转换为 TF 函数，应避免哪些主要模式？\n",
    "\n",
    "9. 何时需要创建动态 Keras 模型？ 如何动态创建Keras模型？为什么不是所有模型都动态化？\n"
   ],
   "id": "f0568883493d9ef7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题",
   "id": "9a282b6d9adda052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 实现一个执行层归一化的自定义层：\n",
    "    - a. `build()` 方法应定义两个可训练的权重 α 和 β，它们的形状均为 `input_shape[-1:]`，数据类型为 `tf.float32`。α 应该用 1 初始化，而 β 必须用 0 初始化。\n",
    "    - b. `call()` 方法应计算每个实例特征的均值和标准差。为此，可以使用 `tf.nn.moments(inputs, axes=-1, keepdims=True)`，它返回同一实例的均值 μ 和方差 σ²（计算方差的平方根便可获得标准差）。然后，该函数应计算并返回\n",
    "      $$\n",
    "      \\alpha \\otimes \\frac{(X-\\mu)}{(\\sigma+\\epsilon)} + \\beta\n",
    "      $$\n",
    "      其中 ε 是表示项精度的一个常量（避免被零除的小常数，例如 0.001）,$\\otimes$表示逐个元素相乘\n",
    "    - c. 确保自定义层产生与tf.keras.layers.LayerNormalization层相同（或几乎相同）的输出。\n",
    "\n",
    "2. 使用自定义训练循环训练模型来处理Fashion MNIST数据集（13_神经网络介绍 里用的数据集）：\n",
    "\n",
    "    - a.显示每个轮次、迭代、平均训练损失和每个轮次的平均精度（在每次迭代中更新），以及每个轮次结束时的验证损失和精度。\n",
    "    - b.尝试对上面的层和下面的层使用具有不同学习率的不同优化器。"
   ],
   "id": "5cd3d7096f87afd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T07:12:24.005353Z",
     "start_time": "2025-09-19T07:12:18.581072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CustomLayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=0.001, **kwargs):\n",
    "        super(CustomLayerNormalization, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 创建可训练参数 alpha (gamma) 和 beta，形状为最后一个维度\n",
    "        shape = input_shape[-1:]\n",
    "        self.alpha = self.add_weight(\n",
    "            name='alpha',\n",
    "            shape=shape,\n",
    "            initializer='ones',  # α 初始化为 1\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=shape,\n",
    "            initializer='zeros',  # β 初始化为 0\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super(CustomLayerNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 计算均值和方差，沿最后一个轴（特征轴），保持维度\n",
    "        mean, variance = tf.nn.moments(inputs, axes=-1, keepdims=True)\n",
    "        std = tf.sqrt(variance + self.epsilon)  # 标准差，加 epsilon 防止除零\n",
    "\n",
    "        # 归一化: (X - μ) / (σ + ε)\n",
    "        normalized = (inputs - mean) / std\n",
    "\n",
    "        # 缩放和平移: α ⊗ normalized + β\n",
    "        return self.alpha * normalized + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomLayerNormalization, self).get_config()\n",
    "        config.update({'epsilon': self.epsilon})\n",
    "        return config"
   ],
   "id": "4cb9da9f5e9ea249",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T07:12:26.350439Z",
     "start_time": "2025-09-19T07:12:26.244219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建测试输入\n",
    "np.random.seed(42)\n",
    "test_input = np.random.randn(4, 10, 8).astype(np.float32)  # (batch, time, features)\n",
    "\n",
    "# 实例化两个层\n",
    "custom_ln = CustomLayerNormalization()\n",
    "official_ln = tf.keras.layers.LayerNormalization(axis=-1, epsilon=0.001)\n",
    "\n",
    "# 构建权重（确保两者使用相同初始化）\n",
    "_ = custom_ln(test_input)\n",
    "_ = official_ln(test_input)\n",
    "\n",
    "# 手动设置官方层的 gamma 和 beta 与自定义层一致（用于精确比较）\n",
    "official_ln.gamma.assign(custom_ln.alpha)\n",
    "official_ln.beta.assign(custom_ln.beta)\n",
    "\n",
    "# 前向传播\n",
    "output_custom = custom_ln(test_input)\n",
    "output_official = official_ln(test_input)\n",
    "\n",
    "# 比较差异\n",
    "diff = tf.reduce_max(tf.abs(output_custom - output_official))\n",
    "print(f\"最大绝对误差: {diff:.6f}\")\n",
    "assert diff < 1e-6, \"输出不一致！\"\n",
    "print(\"✅ 自定义层与官方 LayerNormalization 输出一致！\")"
   ],
   "id": "a179961380ad5a4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大绝对误差: 0.000000\n",
      "✅ 自定义层与官方 LayerNormalization 输出一致！\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T07:26:31.677278Z",
     "start_time": "2025-09-19T07:25:07.005213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ========== 1. 加载并预处理 Fashion MNIST ==========\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "x_train = x_train[..., tf.newaxis]  # (60000, 28, 28, 1)\n",
    "x_test = x_test[..., tf.newaxis]    # (10000, 28, 28, 1)\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "\n",
    "# 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu', name='conv2'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='dense1'),\n",
    "    CustomLayerNormalization(name='custom_ln'),\n",
    "    tf.keras.layers.Dense(10, name='classifier')  # 输出层，无激活（配合 SparseCategoricalCrossentropy(from_logits=True)）\n",
    "])\n",
    "\n",
    "# ========== 3. 定义不同层的优化器 ==========\n",
    "# 分组参数\n",
    "conv_vars = model.get_layer('conv1').trainable_variables + model.get_layer('conv2').trainable_variables\n",
    "dense_vars = model.get_layer('dense1').trainable_variables + model.get_layer('custom_ln').trainable_variables\n",
    "classifier_vars = model.get_layer('classifier').trainable_variables\n",
    "\n",
    "# 为不同组创建不同优化器\n",
    "optimizer_conv = tf.keras.optimizers.Adam(learning_rate=1e-4)   # 底层卷积：小学习率\n",
    "optimizer_dense = tf.keras.optimizers.Adam(learning_rate=1e-3)  # 中间层：中等学习率\n",
    "optimizer_classifier = tf.keras.optimizers.SGD(learning_rate=0.01)  # 分类层：大学习率 + SGD\n",
    "\n",
    "# 损失函数和指标\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# ========== 4. 自定义训练步骤 ==========\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # 分别计算不同组的梯度并应用\n",
    "    grads_conv = tape.gradient(loss, conv_vars)\n",
    "    optimizer_conv.apply_gradients(zip(grads_conv, conv_vars))\n",
    "\n",
    "    grads_dense = tape.gradient(loss, dense_vars)\n",
    "    optimizer_dense.apply_gradients(zip(grads_dense, dense_vars))\n",
    "\n",
    "    grads_classifier = tape.gradient(loss, classifier_vars)\n",
    "    optimizer_classifier.apply_gradients(zip(grads_classifier, classifier_vars))\n",
    "\n",
    "    del tape  # 手动释放 persistent tape\n",
    "    return loss, logits\n",
    "\n",
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    logits = model(x, training=False)\n",
    "    loss = loss_fn(y, logits)\n",
    "    return loss, logits\n",
    "\n",
    "# ========== 5. 数据批处理 ==========\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# ========== 6. 训练循环 ==========\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    train_loss_sum = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # 重置训练精度\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        loss, logits = train_step(x_batch, y_batch)\n",
    "        train_loss_sum += loss\n",
    "        num_batches += 1\n",
    "\n",
    "        # 更新并显示当前 batch 的精度\n",
    "        train_acc_metric.update_state(y_batch, logits)\n",
    "        current_acc = train_acc_metric.result().numpy()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"  Step {step:4d} | Loss: {loss:.4f} | Accuracy: {current_acc:.4f}\")\n",
    "\n",
    "    # 计算平均训练损失和精度\n",
    "    avg_train_loss = train_loss_sum / num_batches\n",
    "    avg_train_acc = train_acc_metric.result().numpy()\n",
    "\n",
    "    # 验证阶段\n",
    "    val_loss_sum = 0.0\n",
    "    val_num_batches = 0\n",
    "    val_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch, y_batch in val_dataset:\n",
    "        loss, logits = val_step(x_batch, y_batch)\n",
    "        val_loss_sum += loss\n",
    "        val_num_batches += 1\n",
    "        val_acc_metric.update_state(y_batch, logits)\n",
    "\n",
    "    avg_val_loss = val_loss_sum / val_num_batches\n",
    "    avg_val_acc = val_acc_metric.result().numpy()\n",
    "\n",
    "    # 显示轮次汇总\n",
    "    print(f\"Epoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Accuracy: {avg_train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Accuracy:   {avg_val_acc:.4f}\")"
   ],
   "id": "7717ab8b885eff7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "  Step    0 | Loss: 2.6239 | Accuracy: 0.1562\n",
      "  Step   50 | Loss: 0.7415 | Accuracy: 0.6330\n",
      "  Step  100 | Loss: 0.5395 | Accuracy: 0.6903\n",
      "  Step  150 | Loss: 0.3453 | Accuracy: 0.7171\n",
      "  Step  200 | Loss: 0.4961 | Accuracy: 0.7387\n",
      "  Step  250 | Loss: 0.3771 | Accuracy: 0.7542\n",
      "  Step  300 | Loss: 0.7706 | Accuracy: 0.7659\n",
      "  Step  350 | Loss: 0.3381 | Accuracy: 0.7763\n",
      "  Step  400 | Loss: 0.5252 | Accuracy: 0.7825\n",
      "  Step  450 | Loss: 0.3935 | Accuracy: 0.7895\n",
      "  Step  500 | Loss: 0.4805 | Accuracy: 0.7940\n",
      "  Step  550 | Loss: 0.4810 | Accuracy: 0.7989\n",
      "  Step  600 | Loss: 0.1966 | Accuracy: 0.8033\n",
      "  Step  650 | Loss: 0.4827 | Accuracy: 0.8081\n",
      "  Step  700 | Loss: 0.3986 | Accuracy: 0.8103\n",
      "  Step  750 | Loss: 0.2668 | Accuracy: 0.8136\n",
      "  Step  800 | Loss: 0.4087 | Accuracy: 0.8164\n",
      "  Step  850 | Loss: 0.6098 | Accuracy: 0.8196\n",
      "  Step  900 | Loss: 0.2995 | Accuracy: 0.8220\n",
      "  Step  950 | Loss: 0.2164 | Accuracy: 0.8239\n",
      "  Step 1000 | Loss: 0.3198 | Accuracy: 0.8255\n",
      "  Step 1050 | Loss: 0.3055 | Accuracy: 0.8283\n",
      "  Step 1100 | Loss: 0.4043 | Accuracy: 0.8308\n",
      "  Step 1150 | Loss: 0.1640 | Accuracy: 0.8321\n",
      "  Step 1200 | Loss: 0.2528 | Accuracy: 0.8342\n",
      "  Step 1250 | Loss: 0.3434 | Accuracy: 0.8356\n",
      "  Step 1300 | Loss: 0.4670 | Accuracy: 0.8369\n",
      "  Step 1350 | Loss: 0.3127 | Accuracy: 0.8381\n",
      "  Step 1400 | Loss: 0.3684 | Accuracy: 0.8390\n",
      "  Step 1450 | Loss: 0.4500 | Accuracy: 0.8401\n",
      "  Step 1500 | Loss: 0.3984 | Accuracy: 0.8412\n",
      "  Step 1550 | Loss: 0.1372 | Accuracy: 0.8423\n",
      "  Step 1600 | Loss: 0.5785 | Accuracy: 0.8429\n",
      "  Step 1650 | Loss: 0.3701 | Accuracy: 0.8438\n",
      "  Step 1700 | Loss: 0.3347 | Accuracy: 0.8451\n",
      "  Step 1750 | Loss: 0.2858 | Accuracy: 0.8457\n",
      "  Step 1800 | Loss: 0.0859 | Accuracy: 0.8465\n",
      "  Step 1850 | Loss: 0.2362 | Accuracy: 0.8475\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 0.4158 | Train Accuracy: 0.8480\n",
      "  Val Loss:   0.3597 | Val Accuracy:   0.8677\n",
      "\n",
      "Epoch 2/3\n",
      "  Step    0 | Loss: 0.3101 | Accuracy: 0.9375\n",
      "  Step   50 | Loss: 0.2983 | Accuracy: 0.8811\n",
      "  Step  100 | Loss: 0.0973 | Accuracy: 0.8883\n",
      "  Step  150 | Loss: 0.1960 | Accuracy: 0.8860\n",
      "  Step  200 | Loss: 0.1637 | Accuracy: 0.8828\n",
      "  Step  250 | Loss: 0.2032 | Accuracy: 0.8850\n",
      "  Step  300 | Loss: 0.2639 | Accuracy: 0.8865\n",
      "  Step  350 | Loss: 0.7485 | Accuracy: 0.8856\n",
      "  Step  400 | Loss: 0.1395 | Accuracy: 0.8858\n",
      "  Step  450 | Loss: 0.4866 | Accuracy: 0.8842\n",
      "  Step  500 | Loss: 0.2790 | Accuracy: 0.8844\n",
      "  Step  550 | Loss: 0.2818 | Accuracy: 0.8852\n",
      "  Step  600 | Loss: 0.2232 | Accuracy: 0.8858\n",
      "  Step  650 | Loss: 0.3730 | Accuracy: 0.8859\n",
      "  Step  700 | Loss: 0.1649 | Accuracy: 0.8869\n",
      "  Step  750 | Loss: 0.3200 | Accuracy: 0.8868\n",
      "  Step  800 | Loss: 0.1529 | Accuracy: 0.8879\n",
      "  Step  850 | Loss: 0.2854 | Accuracy: 0.8886\n",
      "  Step  900 | Loss: 0.1931 | Accuracy: 0.8891\n",
      "  Step  950 | Loss: 0.4134 | Accuracy: 0.8896\n",
      "  Step 1000 | Loss: 0.1871 | Accuracy: 0.8900\n",
      "  Step 1050 | Loss: 0.1399 | Accuracy: 0.8900\n",
      "  Step 1100 | Loss: 0.2373 | Accuracy: 0.8900\n",
      "  Step 1150 | Loss: 0.1799 | Accuracy: 0.8902\n",
      "  Step 1200 | Loss: 0.1633 | Accuracy: 0.8910\n",
      "  Step 1250 | Loss: 0.4868 | Accuracy: 0.8913\n",
      "  Step 1300 | Loss: 0.3853 | Accuracy: 0.8918\n",
      "  Step 1350 | Loss: 0.3118 | Accuracy: 0.8919\n",
      "  Step 1400 | Loss: 0.6116 | Accuracy: 0.8917\n",
      "  Step 1450 | Loss: 0.1865 | Accuracy: 0.8918\n",
      "  Step 1500 | Loss: 0.2327 | Accuracy: 0.8925\n",
      "  Step 1550 | Loss: 0.0457 | Accuracy: 0.8930\n",
      "  Step 1600 | Loss: 0.3158 | Accuracy: 0.8930\n",
      "  Step 1650 | Loss: 0.2172 | Accuracy: 0.8930\n",
      "  Step 1700 | Loss: 0.2817 | Accuracy: 0.8933\n",
      "  Step 1750 | Loss: 0.1186 | Accuracy: 0.8936\n",
      "  Step 1800 | Loss: 0.1943 | Accuracy: 0.8933\n",
      "  Step 1850 | Loss: 0.3125 | Accuracy: 0.8931\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 0.2893 | Train Accuracy: 0.8934\n",
      "  Val Loss:   0.3066 | Val Accuracy:   0.8873\n",
      "\n",
      "Epoch 3/3\n",
      "  Step    0 | Loss: 0.1025 | Accuracy: 1.0000\n",
      "  Step   50 | Loss: 0.1709 | Accuracy: 0.9105\n",
      "  Step  100 | Loss: 0.3044 | Accuracy: 0.9016\n",
      "  Step  150 | Loss: 0.2358 | Accuracy: 0.9044\n",
      "  Step  200 | Loss: 0.1495 | Accuracy: 0.9064\n",
      "  Step  250 | Loss: 0.3651 | Accuracy: 0.9028\n",
      "  Step  300 | Loss: 0.1782 | Accuracy: 0.9049\n",
      "  Step  350 | Loss: 0.3262 | Accuracy: 0.9046\n",
      "  Step  400 | Loss: 0.3110 | Accuracy: 0.9054\n",
      "  Step  450 | Loss: 0.1676 | Accuracy: 0.9052\n",
      "  Step  500 | Loss: 0.2016 | Accuracy: 0.9064\n",
      "  Step  550 | Loss: 0.4148 | Accuracy: 0.9049\n",
      "  Step  600 | Loss: 0.3303 | Accuracy: 0.9042\n",
      "  Step  650 | Loss: 0.0996 | Accuracy: 0.9052\n",
      "  Step  700 | Loss: 0.2045 | Accuracy: 0.9046\n",
      "  Step  750 | Loss: 0.2109 | Accuracy: 0.9048\n",
      "  Step  800 | Loss: 0.3577 | Accuracy: 0.9053\n",
      "  Step  850 | Loss: 0.5164 | Accuracy: 0.9050\n",
      "  Step  900 | Loss: 0.2164 | Accuracy: 0.9051\n",
      "  Step  950 | Loss: 0.1090 | Accuracy: 0.9056\n",
      "  Step 1000 | Loss: 0.0821 | Accuracy: 0.9066\n",
      "  Step 1050 | Loss: 0.3075 | Accuracy: 0.9069\n",
      "  Step 1100 | Loss: 0.0938 | Accuracy: 0.9076\n",
      "  Step 1150 | Loss: 0.1459 | Accuracy: 0.9075\n",
      "  Step 1200 | Loss: 0.1845 | Accuracy: 0.9077\n",
      "  Step 1250 | Loss: 0.2358 | Accuracy: 0.9076\n",
      "  Step 1300 | Loss: 0.1763 | Accuracy: 0.9073\n",
      "  Step 1350 | Loss: 0.5205 | Accuracy: 0.9076\n",
      "  Step 1400 | Loss: 0.1605 | Accuracy: 0.9076\n",
      "  Step 1450 | Loss: 0.3619 | Accuracy: 0.9077\n",
      "  Step 1500 | Loss: 0.2412 | Accuracy: 0.9077\n",
      "  Step 1550 | Loss: 0.1616 | Accuracy: 0.9080\n",
      "  Step 1600 | Loss: 0.3848 | Accuracy: 0.9079\n",
      "  Step 1650 | Loss: 0.1387 | Accuracy: 0.9079\n",
      "  Step 1700 | Loss: 0.3174 | Accuracy: 0.9078\n",
      "  Step 1750 | Loss: 0.1048 | Accuracy: 0.9079\n",
      "  Step 1800 | Loss: 0.4247 | Accuracy: 0.9082\n",
      "  Step 1850 | Loss: 0.1065 | Accuracy: 0.9082\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 0.2460 | Train Accuracy: 0.9084\n",
      "  Val Loss:   0.2829 | Val Accuracy:   0.8987\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
