数据沿用上课使用的数据，没有的可以下载   地址：https://github.com/yt-one/ml-teach/tree/main/datasets

因为随机搜索超参数还没有演示，可以自学做/选做，周一会讲，所以作业周三之前交。

编程作业：

0. 以下作业 要把周五课上的预处理全流程带上 （ColumnTransformer），可以在熟悉了课件里的代码后，考虑自己实现
    实现要求：      
  a. 大多数ML算法不期望缺失值, 数字特征中的缺失值将通过用中位数替换它们来估算，。在分类特征中，缺失值将被最常见的类别替换。
  b. 大多数ML算法只接受数字输入, 类别特征将被独热编码
  c. 计算并添加一些比率特征：bedrooms_ratio、rooms_per_house和people_per_house。希望这些能更好地与房价中位数相关联
  d. 添加集群相似性特征。可能比纬度和经度对模型更有用
  e. 长尾特征被它们的对数取代，因为大多数模型更喜欢具有大致均匀分布或高斯分布的特征。
  f. 大多数ML算法更喜欢所有特征具有大致相同的尺度, 所有数值特征都将被标准化


1. 尝试支持向量机回归器(sklearn.svm.SVR)，用这个模型来做回归。
     试试这个模型的超参数，例如kernel="linear"，kernel="rbf"，不同的kernel选择下也会有不同的超参数​。  分别用GridSearchCV和RandomizedSearchCV探索性能最优（交叉验证后的预测表现最好）的超参数

     请注意，支持向量机不能扩展到大型数据集，因此应该仅在训练集的前5000个实例上训练你的模型，并且仅使用3折交叉验证，否则会要运行很久（按小时计）。
     现在不要担心支持向量机超参数的含义，将在讲支持向量机（SVM）的时候详解

 
2. 去了解sklearn里 SelectFromModel的用法，尝试在数据预处理流水线中添加一个SelectFromModel转换器来仅选择最重要的属性。 并用你想尝试的回归模型去训练数据（线性回归/决策树/随机森林）


3. 周五课堂的随堂练习: .创建一个自定义转换器，在其fit()方法中训练k近邻回归器(sklearn.neighbors.KNeighborsRegressor)，并在其transform()方法中输出模型的预测。然后将此功能添加到预处理流水线，使用纬度和经度作为此转换器的输入。这将在模型中添加一个与最近地区的房价中位数相对应的特征。  添加特征后，用想尝试的回归模型去训练数据。 训练可以采用GridSearchCV和RandomizedSearchCV探索性能最优（交叉验证后的预测表现最好）的超参数



4. 从头开始再次实现StandardScalerClone类（对数据做标准化），然后添加对inverse_transform()方法的支持：执行scaler.inverse_transform(scaler.fit_transform(X))应该返回一个非常接近X的数组。
       然后添加对特征名称的支持：如果输入是DataFrame，则在fit()方法中设置feature_names_in_。该属性类型是NumPy数组，存列的名字。
       最后，实现get_feature_names_out()方法：这个方法应该有一个可选的input_features=None参数。如果传了这个参数，这个方法应检查其长度是否匹配n_features_in_，如果有feature_names_in_属性，则应匹配feature_names_in_的长度，然后应返回input_features。
       如果input_features为None，则该方法应返回feature_names_in_（如果有这个属性）​，否则返回长度为n_features_in_的   np.array(["x0"，"x1"，"x2", ...])。